{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import random\n",
    "import codecs\n",
    "# this input_path is just example\n",
    "#inputpath = \"C:/Users/Administrator/Desktop/Smote/describledata/kddcup.data_10_percent_corrected\"\n",
    "Nor_list = ['normal']\n",
    "Dos_list = ['back', 'land', 'neptune' , 'pod', 'smurf', 'teardrop', 'mailbomb', 'processtable', 'udpstorm', 'apache2', 'worm']\n",
    "R2L_list =['guess_passwd', 'ftp_write', 'imap', 'phf', 'multihop', 'warezmaster', 'warezclient', 'xlock','xsnoop', 'snmpguess', 'snmpgetattack', 'httptunnel','spy','named','snmpguess']\n",
    "Probe_list = ['satan','ipsweep', 'nmap', 'portsweep', 'mscan', 'saint']\n",
    "U2R_list =['buffer_overflow', 'loadmodule', 'rootkit', 'perl', 'sqlattack', 'xterm', 'ps','httptunnel']\n",
    "continue_col_index = [0,4,5,7,8,9,10,12,15,16,17,18,19,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40]\n",
    "decreate_col_index =[1,2,3,6,11,13,14,20,21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_lable1(seg):\n",
    "    if seg in Nor_list:\n",
    "        return  0\n",
    "    elif seg in Dos_list:\n",
    "        return  1\n",
    "    elif seg in R2L_list:\n",
    "        return  2\n",
    "    elif seg in Probe_list:\n",
    "        return 3\n",
    "    elif seg in U2R_list:      \n",
    "        return  4\n",
    "    \n",
    "    \n",
    "def gen_label2(seg):\n",
    "    if seg in Nor_list:\n",
    "        return  0\n",
    "    else:\n",
    "        return 1\n",
    "def replace(columnlist,data):\n",
    "    #离散值代替\n",
    "    #listt = []\n",
    "    for i in columnlist:\n",
    "        listt = []\n",
    "        for line,seg in enumerate(data[:,i]):\n",
    "            if seg in listt:\n",
    "                data[line][i] = listt.index(seg)\n",
    "            else:\n",
    "                listt.append(seg)\n",
    "                data[line][i] = listt.index(seg)\n",
    "\n",
    "def processdata(file_path):\n",
    "    tmp = None\n",
    "    with codecs.open(file_path,'r') as f:\n",
    "        content = f.readlines()\n",
    "        datas =[]\n",
    "        for line in content:\n",
    "            line = line.strip()\n",
    "            line = line[:-1].split(',')\n",
    "            datas.append(line)\n",
    "        datas = np.array(datas)\n",
    "        replace(decreate_col_index,datas)\n",
    "        new_datas =[]\n",
    "        for index,col in enumerate(datas):\n",
    "            new_datas.append([float(k) for k in col[:-1]])\n",
    "            if random.random()<0.00005:\n",
    "                print(col[-1])\n",
    "            new_datas[index].append(gen_lable1(col[-1]))\n",
    "            new_datas[index].append(gen_label2(col[-1]))\n",
    "        tmp = new_datas\n",
    "    datas = np.array(tmp).astype('float32')\n",
    "    \n",
    "    for j in continue_col_index:\n",
    "        meanVal=np.mean(datas[:,j])\n",
    "        stdVal=np.std(datas[:,j])\n",
    "        datas[:,j]=(datas[:,j]-meanVal)/stdVal\n",
    "    return datas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal\nsmurf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neptune\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neptune\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smurf\nsmurf\nsmurf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smurf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smurf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smurf\nsmurf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smurf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neptune\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neptune\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smurf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smurf\nsmurf\nnormal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:55: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "data = processdata('./describledata/kddcup.data_10_percent_corrected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smurf\nsmurf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neptune\nneptune\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smurf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smurf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smurf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neptune\nneptune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:55: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "label_dict ={}\n",
    "for i in data[:,-2]:\n",
    "    if i in label_dict.keys():\n",
    "        continue\n",
    "    else:\n",
    "        label_dict[i] = len(label_dict)\n",
    "test_data = processdata('./describledata/corrected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan(data):\n",
    "    #m,n = data.shape\n",
    "    choice_list =[]\n",
    "    matrix = np.isnan(data)\n",
    "    for m in range(len(data)):\n",
    "        for n in range(len(data[0])):\n",
    "            if matrix[m,n] ==True:\n",
    "                data[m][n] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fill_nan(data)\n",
    "fill_nan(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import math\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "class Smote():\n",
    "    def __init__(self,distance,range1,range2):\n",
    "        self.synthetic_arr = []\n",
    "        self.newindex = 0\n",
    "        self.distance_measure = distance\n",
    "        self.range1 =range1\n",
    "        self.range2 = range2\n",
    "\n",
    "    def Populate(self, N, i, indices, min_samples, k):\n",
    "        \"\"\"\n",
    "            此代码主要作用是生成增强数组\n",
    "\n",
    "            Returns:返回增强后的数组\n",
    "        \"\"\"\n",
    "\n",
    "        choice_list =[]\n",
    "        def choice(data):\n",
    "            p = []\n",
    "            wc = {}\n",
    "            for num in data:\n",
    "                # print(num)\n",
    "                wc[num] = wc.setdefault(num, 0) + 1\n",
    "            for key in wc.keys():\n",
    "                p.append(wc[key] / len(data))\n",
    "            # print(p)\n",
    "            keylist = np.array([key for key in wc.keys()])\n",
    "            # print(wc[0])\n",
    "            return keylist,p\n",
    "        for index in self.range1:\n",
    "            choice_list.append(choice(min_samples[:,index]))\n",
    "\n",
    "        while N != 0:\n",
    "            arr = np.zeros(len(min_samples[0]))\n",
    "            arr[-2] = min_samples[i][-2]\n",
    "            arr[-1] = min_samples[i][-1]\n",
    "            nn = randint(0, k - 2)\n",
    "            # 统计离散型变量\n",
    "            for rowindex,index in enumerate(self.range1):\n",
    "                arr[index] = np.random.choice(choice_list[rowindex][0],size=1,p=choice_list[rowindex][1])\n",
    "            #for attr in features2:\n",
    "            for attr in self.range2:\n",
    "                min_samples[i][attr] = float(min_samples[i][attr])\n",
    "                min_samples[indices[nn]][attr] = float(min_samples[indices[nn]][attr])\n",
    "                try:\n",
    "                    diff = float(min_samples[indices[nn]][attr]) - float(min_samples[i][attr])\n",
    "                except:\n",
    "                    print('这是第%d列'%attr,min_samples[indices[nn]][attr],min_samples[i][attr])\n",
    "                gap = random.uniform(0, 1)\n",
    "\n",
    "                arr[attr] = float(min_samples[i][attr]) + gap * diff\n",
    "            #print(arr)\n",
    "            self.synthetic_arr.append(arr)\n",
    "            self.newindex = self.newindex + 1\n",
    "            N = N - 1\n",
    "\n",
    "    def k_neighbors(self, euclid_distance, k):\n",
    "        nearest_idx_npy = np.empty([euclid_distance.shape[0], euclid_distance.shape[0]], dtype=np.int64)\n",
    "\n",
    "        for i in range(len(euclid_distance)):\n",
    "            idx = np.argsort(euclid_distance[i])\n",
    "            nearest_idx_npy[i] = idx\n",
    "            idx = 0\n",
    "\n",
    "        return nearest_idx_npy[:, 1:k]\n",
    "\n",
    "    def find_k(self, X, k):\n",
    "\n",
    "        \"\"\"\n",
    "               Finds k nearest neighbors using euclidian distance\n",
    "\n",
    "               Returns: The k nearest neighbor\n",
    "        \"\"\"\n",
    "\n",
    "        euclid_distance = np.empty([X.shape[0], X.shape[0]], dtype=np.float32)\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            dist_arr = []\n",
    "            for j in range(len(X)):\n",
    "                dist_arr.append(math.sqrt(sum((X[j] - X[i]) ** 2)))\n",
    "            dist_arr = np.asarray(dist_arr, dtype=np.float32)\n",
    "            euclid_distance[i] = dist_arr\n",
    "\n",
    "        return self.k_neighbors(euclid_distance, k)\n",
    "\n",
    "    def generate_synthetic_points(self, min_samples, N, k):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            min_samples : 要增强的数据\n",
    "            N :要额外生成的负样本的数目\n",
    "            k : int. Number of nearest neighbours.\n",
    "            Returns\n",
    "            -------\n",
    "            S : Synthetic samples. array,\n",
    "                shape = [(N/100) * n_minority_samples, n_features].\n",
    "        \"\"\"\n",
    "\n",
    "        if N < 1:\n",
    "            raise ValueError(\"Value of N cannot be less than 100%\")\n",
    "\n",
    "        if self.distance_measure not in ('euclidian', 'ball_tree'):\n",
    "            raise ValueError(\"Invalid Distance Measure.You can use only Euclidian or ball_tree\")\n",
    "\n",
    "        if k > min_samples.shape[0]:\n",
    "            raise ValueError(\"Size of k cannot exceed the number of samples.\")\n",
    "\n",
    "        T = min_samples.shape[0]\n",
    "\n",
    "        if self.distance_measure == 'euclidian':\n",
    "            indices = self.find_k(min_samples, k)\n",
    "\n",
    "        elif self.distance_measure == 'ball_tree':\n",
    "            nb = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(min_samples)\n",
    "            distance, indices = nb.kneighbors(min_samples)\n",
    "            indices = indices[:, 1:]\n",
    "\n",
    "        for i in range(indices.shape[0]):\n",
    "            self.Populate(N, i, indices[i], min_samples, k)\n",
    "\n",
    "        return np.asarray(self.synthetic_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rowindex = []\n",
    "for i, line in enumerate(data):\n",
    "    if line[-2] ==4.0:\n",
    "        rowindex.append(i)\n",
    "range1 = [1,2,3,6,11,13,14,20,21]\n",
    "range2 = [0,4,5,7,8,9,10,12,15,16,17,18,19,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40]\n",
    "minsamples = data[rowindex]\n",
    "smote = Smote(distance='ball_tree',range1=range1,range2=range2)\n",
    "smotedata = smote.generate_synthetic_points(min_samples=minsamples,N=100,k=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5200, 43)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smotedata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "adddata = np.concatenate((smotedata,data),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499221, 43)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adddata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_nan(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  保存数据\n",
    "\"\"\"\n",
    "np.save('data.npz',data)\n",
    "np.save('adddata.npz',adddata)\n",
    "np.save('testdata.npz',test_data)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import random\n",
    "import codecs\n",
    "data = np.load('data.npz.npy')\n",
    "adddata =np.load('adddata.npz.npy')\n",
    "test_data =np.load('testdata.npz.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_train_test(dat):        #分割训练集和测试集\n",
    "    index = int(len(dat)*0.8)\n",
    "    np.random.shuffle(dat)\n",
    "    traindat=torch.from_numpy(dat[:index,:-2]).float()\n",
    "    trainlabel=torch.from_numpy(dat[:index,-2]).long()\n",
    "    validdat = torch.from_numpy(dat[index:,:-2]).float()\n",
    "    validlabel = torch.from_numpy(dat[index:,-2]).long()\n",
    "    # print(dat[0])\n",
    "    return traindat,trainlabel,validdat,validlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "from  DBN import DBN\n",
    "import torch.utils.data as Data\n",
    "def train_batch(traind,trainl,SIZE=500,SHUFFLE=True):   #分批处理\n",
    "    trainset=Data.TensorDataset(traind,trainl)\n",
    "    trainloader=Data.DataLoader(\n",
    "        dataset=trainset,\n",
    "        batch_size=SIZE,\n",
    "        shuffle=SHUFFLE)\n",
    "    return trainloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindat,trainlabel, validdat, validlabel = gen_train_test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = train_batch(traindat,trainlabel)\n",
    "from torch.autograd.variable import  Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the 1 st rbm layer"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\torch\\nn\\functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Error(epoch:0) : 45406564.0000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "dbn=DBN(visible_units=len(traindat[0]))\n",
    "dbn.to(device)\n",
    "dbn.train()\n",
    "dbn.train_static(train_data=traindat,train_labels=trainlabel,num_epochs=3,batch_size=20)\n",
    "\n",
    "optimizer = torch.optim.SGD(dbn.parameters(), lr=0.001, momentum=0.9)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "dbn.trainBP(trainloader)\n",
    "\n",
    "for epoch in range(5):\n",
    "    for step,(x,y) in tqdm(enumerate(trainloader)):\n",
    "        # print(x.data.numpy(),y.data.numpy())\n",
    "\n",
    "        b_x=Variable(x)\n",
    "        b_y=Variable(y)\n",
    "\n",
    "        output=dbn(b_x)\n",
    "        # print(output)\n",
    "        # print(prediction);print(output);print(b_y)\n",
    "\n",
    "        loss=loss_func(output,b_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step%10==0:\n",
    "            print('Epoch: ', epoch, 'step:',step,'| train loss: %.4f' % loss.data.numpy())\n",
    "duration=time.time()-start_time\n",
    "\n",
    "dbn.eval()\n",
    "test_x = Variable(validdat);test_y = Variable(validlabel)\n",
    "test_out = dbn(test_x)\n",
    "# print(test_out)\n",
    "test_pred = torch.max(test_out, 1)[1]\n",
    "pre_val = test_pred.data.squeeze().numpy()\n",
    "y_val = test_y.data.squeeze().numpy()\n",
    "print('prediciton:',pre_val);print('true value:',y_val)\n",
    "accuracy = float((pre_val == y_val).astype(int).sum()) / float(test_y.size(0))\n",
    "print('test accuracy: %.2f' % accuracy,'duration:%.4f' % duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the 1 st rbm layer"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\torch\\nn\\functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-92b033689ab5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraindat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaliddat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-f0d6004f9c4d>\u001b[0m in \u001b[0;36mtrain_and_test\u001b[0;34m(traind, trainl, testdat, testlabel, loader)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_static\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Administrator\\Documents\\GitHub\\dbn_pytorch\\DBN.py\u001b[0m in \u001b[0;36mtrain_static\u001b[0;34m(self, train_data, train_labels, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0m_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrbm_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# print(train_data.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Administrator\\Documents\\GitHub\\dbn_pytorch\\RBM.py\u001b[0m in \u001b[0;36mtrains\u001b[0;34m(self, train_data, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                 \u001b[0mbatch_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mepoch_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Administrator\\Documents\\GitHub\\dbn_pytorch\\RBM.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    156\u001b[0m         '''\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# print('w:',self.weight);print('b:',self.b);print('c:',self.c)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrastive_divergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Administrator\\Documents\\GitHub\\dbn_pytorch\\RBM.py\u001b[0m in \u001b[0;36mcontrastive_divergence\u001b[0;34m(self, input_data, training)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mhidden_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpositive_hidden_act\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0;31m#采样步数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mvisible_p\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_activations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mhidden_probabilities\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisible_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Administrator\\Documents\\GitHub\\dbn_pytorch\\RBM.py\u001b[0m in \u001b[0;36mto_visible\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# 计算隐含层激活，然后转换为概率\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# print('vinput:',X)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mX_dash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mX_dash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dash\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m#W.T*x+b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# print('mm:',X_dash)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
